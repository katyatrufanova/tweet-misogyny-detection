{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install folium urllib3 transformers keras tensorflow\n"
      ],
      "metadata": {
        "id": "hlDDA6O5r3hQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc50e1cf-b5cc-401e-bef1-86aca928bf8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: folium in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (1.26.16)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: branca>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from folium) (0.6.0)\n",
            "Requirement already satisfied: jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from folium) (3.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from folium) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from folium) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.10)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.6.3)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.9->folium) (2.1.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->folium) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->folium) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->folium) (3.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import re\n",
        "from google.colab import drive\n",
        "from transformers import AutoTokenizer, RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
      ],
      "metadata": {
        "id": "9yVF7N_9r4kQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AQDRHahwr8fR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a867284c-1928-44cd-9860-911e965fcbc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_folder = '/content/drive/My Drive/NLP/AMI2020/'\n",
        "destination_folder = '/content/drive/My Drive/ami_umberto/'\n",
        "\n",
        "MAX_LEN = 128\n",
        "batch_size = 16\n",
        "epochs = 8"
      ],
      "metadata": {
        "id": "xlzae-XMr-1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "def load_and_preprocess_data(source_folder):\n",
        "    df = pd.read_csv(source_folder + \"trainingset/AMI2020_training_raw_anon.tsv\", delimiter='\\t', header=0, names=['id', 'text', 'misoginous', 'aggressiveness'])\n",
        "    df['text'] = df['text'].apply(lambda x: re.sub(r'<MENTION_\\d+>|<URL>', '', x))\n",
        "    sentences = df.text.values\n",
        "    labels = df.misoginous.values\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"Musixmatch/umberto-commoncrawl-cased-v1\")\n",
        "\n",
        "    input_ids = [tokenizer.encode(sent, add_special_tokens=True) for sent in sentences]\n",
        "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=1, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "    attention_masks = [[int(token_id > 1) for token_id in sent] for sent in input_ids]\n",
        "\n",
        "    return input_ids, attention_masks, labels, tokenizer"
      ],
      "metadata": {
        "id": "vGDdT297sARq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and validation sets\n",
        "def split_data(input_ids, attention_masks, labels):\n",
        "    train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=2018, test_size=0.1)\n",
        "    train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=2018, test_size=0.1)\n",
        "\n",
        "    train_inputs = torch.tensor(train_inputs)\n",
        "    validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "    train_labels = torch.tensor(train_labels)\n",
        "    validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "    train_masks = torch.tensor(train_masks)\n",
        "    validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "    return train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks"
      ],
      "metadata": {
        "id": "GQtnvClBsC-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataloaders\n",
        "def create_dataloaders(train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks, batch_size):\n",
        "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "    validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "    validation_sampler = SequentialSampler(validation_data)\n",
        "    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "    return train_dataloader, validation_dataloader"
      ],
      "metadata": {
        "id": "ax4H4Q6csFCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "metadata": {
        "id": "Ay73Lg1AsG8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "def train_model(train_dataloader, validation_dataloader, epochs):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = RobertaForSequenceClassification.from_pretrained(\"Musixmatch/umberto-commoncrawl-cased-v1\", num_labels=2, output_attentions=False, output_hidden_states=False)\n",
        "    model.cuda()\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "    seed_val = 42\n",
        "    random.seed(seed_val)\n",
        "    np.random.seed(seed_val)\n",
        "    torch.manual_seed(seed_val)\n",
        "    torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "    for epoch_i in range(0, epochs):\n",
        "        # Training\n",
        "        total_train_loss = 0\n",
        "        model.train()\n",
        "        t0 = time.time()\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            if step % 40 == 0 and not step == 0:\n",
        "                elapsed = format_time(time.time() - t0)\n",
        "                print('Epoch {:}/{:}, Batch {:}/{:}, Elapsed: {:}.'.format(epoch_i + 1, epochs, step, len(train_dataloader), elapsed,))\n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "            b_labels = batch[2].to(device)\n",
        "\n",
        "            model.zero_grad()\n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "            loss = outputs[0]\n",
        "            total_train_loss += loss.item()\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "        training_time = format_time(time.time() - t0)\n",
        "        print(\"Epoch {:}/{:}, Average training loss: {:.4f}, Training epoch time: {:}\".format(epoch_i + 1, epochs, avg_train_loss, training_time))\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        total_eval_accuracy = 0\n",
        "        total_eval_loss = 0\n",
        "        nb_eval_steps = 0\n",
        "        t0 = time.time()\n",
        "\n",
        "        for batch in validation_dataloader:\n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "            b_labels = batch[2].to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "\n",
        "            loss = outputs[0]\n",
        "            logits = outputs[1]\n",
        "\n",
        "            total_eval_loss += loss.item()\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "        validation_time = format_time(time.time() - t0)\n",
        "        print(\"Epoch {:}/{:}, Average validation loss: {:.4f}, Average validation accuracy: {:.4f}, Validation epoch time: {:}\".format(epoch_i + 1, epochs, avg_val_loss, avg_val_accuracy, validation_time))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "dupF4ULYsMzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids, attention_masks, labels, tokenizer = load_and_preprocess_data(source_folder)\n",
        "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = split_data(input_ids, attention_masks, labels)\n",
        "train_dataloader, validation_dataloader = create_dataloaders(train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks, batch_size)\n",
        "model = train_model(train_dataloader, validation_dataloader, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilBanz0gr1gv",
        "outputId": "60acdaa0-09d2-4f57-c3e1-602c67cc7f0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type camembert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of the model checkpoint at Musixmatch/umberto-commoncrawl-cased-v1 were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at Musixmatch/umberto-commoncrawl-cased-v1 and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8, Batch 40/248, Elapsed: 0:00:13.\n",
            "Epoch 1/8, Batch 80/248, Elapsed: 0:00:25.\n",
            "Epoch 1/8, Batch 120/248, Elapsed: 0:00:38.\n",
            "Epoch 1/8, Batch 160/248, Elapsed: 0:00:52.\n",
            "Epoch 1/8, Batch 200/248, Elapsed: 0:01:05.\n",
            "Epoch 1/8, Batch 240/248, Elapsed: 0:01:18.\n",
            "Epoch 1/8, Average training loss: 0.4330, Training epoch time: 0:01:21\n",
            "Epoch 1/8, Average validation loss: 0.2951, Average validation accuracy: 0.8884, Validation epoch time: 0:00:03\n",
            "Epoch 2/8, Batch 40/248, Elapsed: 0:00:13.\n",
            "Epoch 2/8, Batch 80/248, Elapsed: 0:00:27.\n",
            "Epoch 2/8, Batch 120/248, Elapsed: 0:00:40.\n",
            "Epoch 2/8, Batch 160/248, Elapsed: 0:00:54.\n",
            "Epoch 2/8, Batch 200/248, Elapsed: 0:01:07.\n",
            "Epoch 2/8, Batch 240/248, Elapsed: 0:01:21.\n",
            "Epoch 2/8, Average training loss: 0.2629, Training epoch time: 0:01:24\n",
            "Epoch 2/8, Average validation loss: 0.2915, Average validation accuracy: 0.8772, Validation epoch time: 0:00:03\n",
            "Epoch 3/8, Batch 40/248, Elapsed: 0:00:14.\n",
            "Epoch 3/8, Batch 80/248, Elapsed: 0:00:28.\n",
            "Epoch 3/8, Batch 120/248, Elapsed: 0:00:41.\n",
            "Epoch 3/8, Batch 160/248, Elapsed: 0:00:55.\n",
            "Epoch 3/8, Batch 200/248, Elapsed: 0:01:09.\n",
            "Epoch 3/8, Batch 240/248, Elapsed: 0:01:23.\n",
            "Epoch 3/8, Average training loss: 0.2006, Training epoch time: 0:01:26\n",
            "Epoch 3/8, Average validation loss: 0.2656, Average validation accuracy: 0.9085, Validation epoch time: 0:00:03\n",
            "Epoch 4/8, Batch 40/248, Elapsed: 0:00:14.\n",
            "Epoch 4/8, Batch 80/248, Elapsed: 0:00:28.\n",
            "Epoch 4/8, Batch 120/248, Elapsed: 0:00:42.\n",
            "Epoch 4/8, Batch 160/248, Elapsed: 0:00:56.\n",
            "Epoch 4/8, Batch 200/248, Elapsed: 0:01:11.\n",
            "Epoch 4/8, Batch 240/248, Elapsed: 0:01:25.\n",
            "Epoch 4/8, Average training loss: 0.1575, Training epoch time: 0:01:27\n",
            "Epoch 4/8, Average validation loss: 0.2838, Average validation accuracy: 0.9085, Validation epoch time: 0:00:03\n",
            "Epoch 5/8, Batch 40/248, Elapsed: 0:00:14.\n",
            "Epoch 5/8, Batch 80/248, Elapsed: 0:00:28.\n",
            "Epoch 5/8, Batch 120/248, Elapsed: 0:00:42.\n",
            "Epoch 5/8, Batch 160/248, Elapsed: 0:00:56.\n",
            "Epoch 5/8, Batch 200/248, Elapsed: 0:01:10.\n",
            "Epoch 5/8, Batch 240/248, Elapsed: 0:01:25.\n",
            "Epoch 5/8, Average training loss: 0.1266, Training epoch time: 0:01:27\n",
            "Epoch 5/8, Average validation loss: 0.3219, Average validation accuracy: 0.9062, Validation epoch time: 0:00:03\n",
            "Epoch 6/8, Batch 40/248, Elapsed: 0:00:14.\n",
            "Epoch 6/8, Batch 80/248, Elapsed: 0:00:28.\n",
            "Epoch 6/8, Batch 120/248, Elapsed: 0:00:42.\n",
            "Epoch 6/8, Batch 160/248, Elapsed: 0:00:56.\n",
            "Epoch 6/8, Batch 200/248, Elapsed: 0:01:10.\n",
            "Epoch 6/8, Batch 240/248, Elapsed: 0:01:25.\n",
            "Epoch 6/8, Average training loss: 0.1050, Training epoch time: 0:01:27\n",
            "Epoch 6/8, Average validation loss: 0.3385, Average validation accuracy: 0.9134, Validation epoch time: 0:00:03\n",
            "Epoch 7/8, Batch 40/248, Elapsed: 0:00:14.\n",
            "Epoch 7/8, Batch 80/248, Elapsed: 0:00:28.\n",
            "Epoch 7/8, Batch 120/248, Elapsed: 0:00:42.\n",
            "Epoch 7/8, Batch 160/248, Elapsed: 0:00:56.\n",
            "Epoch 7/8, Batch 200/248, Elapsed: 0:01:11.\n",
            "Epoch 7/8, Batch 240/248, Elapsed: 0:01:25.\n",
            "Epoch 7/8, Average training loss: 0.0847, Training epoch time: 0:01:27\n",
            "Epoch 7/8, Average validation loss: 0.3493, Average validation accuracy: 0.9090, Validation epoch time: 0:00:03\n",
            "Epoch 8/8, Batch 40/248, Elapsed: 0:00:14.\n",
            "Epoch 8/8, Batch 80/248, Elapsed: 0:00:28.\n",
            "Epoch 8/8, Batch 120/248, Elapsed: 0:00:42.\n",
            "Epoch 8/8, Batch 160/248, Elapsed: 0:00:56.\n",
            "Epoch 8/8, Batch 200/248, Elapsed: 0:01:11.\n",
            "Epoch 8/8, Batch 240/248, Elapsed: 0:01:25.\n",
            "Epoch 8/8, Average training loss: 0.0766, Training epoch time: 0:01:27\n",
            "Epoch 8/8, Average validation loss: 0.3529, Average validation accuracy: 0.9129, Validation epoch time: 0:00:03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model():\n",
        "    df = pd.read_csv(source_folder + \"testset/AMI2020_test_raw_gold_anon.tsv\", delimiter='\\t', header=0, names=['id', 'sentence', 'label', 'aggressiveness'])\n",
        "    sentences = df.sentence.values\n",
        "    labels = df.label.values\n",
        "\n",
        "    input_ids = []\n",
        "    for sent in sentences:\n",
        "        encoded_sent = tokenizer.encode(\n",
        "            sent,\n",
        "            add_special_tokens=True,\n",
        "        )\n",
        "        input_ids.append(encoded_sent)\n",
        "\n",
        "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\", value=1)\n",
        "\n",
        "    attention_masks = [[float(i > 1) for i in seq] for seq in input_ids]\n",
        "\n",
        "    prediction_inputs = torch.tensor(input_ids)\n",
        "    prediction_masks = torch.tensor(attention_masks)\n",
        "    prediction_labels = torch.tensor(labels)\n",
        "\n",
        "    batch_size = 16\n",
        "\n",
        "    prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "    prediction_sampler = SequentialSampler(prediction_data)\n",
        "    prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
        "\n",
        "    print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    for batch in prediction_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "        logits = outputs[0]\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        predictions.extend(logits)\n",
        "        true_labels.extend(label_ids)\n",
        "\n",
        "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
        "\n",
        "    def eval_accuracy(a, b):\n",
        "        true_pred = [j for i, j in zip(a, b) if i == j]\n",
        "        accuracy = len(true_pred) / len(a)\n",
        "        return accuracy\n",
        "\n",
        "    print(eval_accuracy(true_labels, pred_flat))"
      ],
      "metadata": {
        "id": "VnyDFcQed50a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "uJY1eLUxg5L2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPK5wO9_dF8x",
        "outputId": "5f09277f-f4fa-4315-e3b0-f1ec77803b1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting labels for 1,000 test sentences...\n",
            "0.821\n"
          ]
        }
      ]
    }
  ]
}